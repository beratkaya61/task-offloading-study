# LLM Few-Shot Prompting Strategy

## ğŸ¯ Hedef: Rule-Based Fallback'i Minimize Etme

### Stratejik YaklaÅŸÄ±m

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IoT Task Offloading Analyzer          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Primary Path: TinyLlama (Few-Shot)     â”‚
â”‚         â†“                               â”‚
â”‚    Success? â†’ Return LLM Analysis       â”‚
â”‚         â†“                               â”‚
â”‚    Failure? â†’ Fallback to Rule-Based    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Neden TinyLlama?

| Ã–zellik          | distilgpt2  | TinyLlama         | Avantaj                          |
| ---------------- | ----------- | ----------------- | -------------------------------- |
| **Parametre**    | 82M         | 1.1B              | 13x daha bÃ¼yÃ¼k = daha iyi anlama |
| **EÄŸitim**       | Causal LM   | Instruction-Tuned | Talimatlara dikkat eder          |
| **Chat FormatÄ±** | âŒ          | âœ…                | Ä°nsan gibi talimatlar izler      |
| **HÄ±z**          | HÄ±zlÄ± (CPU) | Makul (GPU ideal) | KÃ¼Ã§Ã¼k cihazlarda Ã§alÄ±ÅŸÄ±r         |
| **Kalite**       | DÃ¼ÅŸÃ¼k       | YÃ¼ksek            | Ã‡ok daha iyi Ã§Ä±ktÄ±               |

---

## ğŸ“‹ Few-Shot Prompting Nedir?

### Konsept: Ã–rneklerle Ã–ÄŸretme

```python
# KÃ–TÃœ: Model ne yapmasÄ± gerektiÄŸini bilmiyor
prompt = "Task Type: CRITICAL, Size: 1MB, Priority Score: ?"

# Ä°YÄ°: Modele Ã¶rnekler gÃ¶steriyoruz (Few-Shot)
prompt = """
[EXAMPLE 1] CRITICAL â†’ Priority: 0.85, Recommendation: EDGE
[EXAMPLE 2] HIGH_DATA â†’ Priority: 0.65, Recommendation: CLOUD
[EXAMPLE 3] BEST_EFFORT â†’ Priority: 0.25, Recommendation: LOCAL

Now analyze: [NEW TASK]
"""
```

### Neden Few-Shot Ã‡alÄ±ÅŸÄ±r?

1. **Format TutarlÄ±lÄ±ÄŸÄ±**: Model Ã¶rnek yapÄ±yÄ± takip eder
2. **DoÄŸru AralÄ±klar**: 0-1 aralÄ±ÄŸÄ±nda sayÄ± yazmasÄ±nÄ± Ã¶ÄŸrenmiÅŸ
3. **Ä°ÅŸ MantÄ±ÄŸÄ±**: GÃ¶rev Ã¶zelliklerini recommendation ile baÄŸlayan pattern'i sÄ±naÅŸtÄ±rÄ±r
4. **Hata OranÄ± DÃ¼ÅŸer**: Hallucination (saÃ§ma Ã§Ä±ktÄ±) minimize edilir

---

## ğŸ”§ Uygulama DetaylarÄ±

### 1. Few-Shot Examples (llm_analyzer.py satÄ±r ~160-190)

```python
few_shot_examples = """
[EXAMPLE 1]
Input: Task Type: CRITICAL, Size: 1.50 MB, CPU: 0.50 GHz, Deadline: 0.50 seconds
Analysis:
- Priority Score: 0.85 (CRITICAL tasks need immediate response)
- Recommendation: EDGE (Critical tasks benefit from low latency)

[EXAMPLE 2]
Input: Task Type: HIGH_DATA, Size: 50.00 MB, CPU: 10.00 GHz, Deadline: 5.00 seconds
Analysis:
- Priority Score: 0.65 (High data workload)
- Recommendation: CLOUD (Complex computation exceeds edge capacity)

[EXAMPLE 3]
Input: Task Type: BEST_EFFORT, Size: 0.10 MB, CPU: 0.01 GHz, Deadline: 10.00 seconds
Analysis:
- Priority Score: 0.25 (Low priority)
- Recommendation: LOCAL (Minimal resource requirement)
"""
```

**Ä°Ã§ MantÄ±k:**

- 3 farklÄ± gÃ¶rev tÃ¼rÃ¼ Ã¶rneÄŸi
- Her Ã¶rnek tam aÃ§Ä±klama ile
- Model bu pattern'i yeni gÃ¶revlere uyguluyor

### 2. Parsing & Validation (llm_analyzer.py satÄ±r ~220-260)

```python
def _parse_llm_response(self, analysis_text, task):
    # Regex ile score'larÄ± Ã§Ä±kar
    priority_match = re.search(r"Priority Score:\s*([\d.]+)", analysis_text)

    # AralÄ±k doÄŸrulamasÄ±
    if not (0 <= priority_score <= 1):
        return None  # HatalÄ± â†’ fallback'e gÃ¶nder

    # Recommendation validation
    if recommended_target not in ["local", "edge", "cloud"]:
        return None  # TanÄ±madÄ±ÄŸÄ± seÃ§enek â†’ fallback
```

**GÃ¼venlik KatmanlarÄ±:**

1. Regex match baÅŸarÄ±sÄ±z â†’ `None` â†’ rule-based
2. Score aralÄ±k dÄ±ÅŸÄ± â†’ `None` â†’ rule-based
3. Bilinmeyen recommendation â†’ `None` â†’ rule-based

### 3. BaÅŸarÄ± Takibi

```python
self.llm_success_count = 0         # TinyLlama baÅŸarÄ±lÄ± oldu
self.rule_based_fallback_count = 0 # Rule-based'e geri dÃ¶ndÃ¼

# Her analiz sonunda:
if parsed:
    self.llm_success_count += 1
    print("[LLM] âœ“ Successful analysis")
else:
    self.rule_based_fallback_count += 1
    print("[LLM] âœ— Using rule-based fallback")
```

---

## ğŸ“Š Beklenen SonuÃ§lar

### BaÅŸarÄ± OranÄ± Projeksiyonu

| Senaryo         | Rule-Based Fallback OranÄ±          |
| --------------- | ---------------------------------- |
| **Ä°lk BaÅŸta**   | ~30-40% (parsing hatalarÄ±)         |
| **Sonra**       | ~5-10% (edge cases, hallucination) |
| **Uzun Vadede** | ~2-3% (nadiren)                    |

### Neden AzalÄ±r?

1. **Few-Shot Etkisi GÃ¼Ã§lenir**: Model pattern'i daha iyi kaplÄ±yor
2. **Validation SÄ±klaÅŸtÄ±rÄ±labilir**: EÄŸer hala fallback varsa, few-shot'a yeni Ã¶rnekler eklenebilir
3. **Daha Ä°yi Model**: Gerekirse daha bÃ¼yÃ¼k model (Llama 7B) kullanÄ±labilir

---

## ğŸš€ BaÅŸlatma

### Ä°lk Kez Ã‡alÄ±ÅŸtÄ±rma

```bash
# TinyLlama'yÄ± indir ve cache'le (~5GB)
python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0'); AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')"

# Simulasyon baÅŸlat (LLM etkin)
.\run_simulation.bat
```

### Enable/Disable

```python
# gui.py veya train_agent.py'da
analyzer = SemanticAnalyzer(
    use_llm=True,  # â† Bu satÄ±rÄ± False yaparak rule-based geÃ§ebilirsiniz
    model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
)
```

---

## ğŸ“ˆ Performans Metrikleri

### GUI'de GÃ¶rÃ¼necek Bilgiler

```
[LLM] âœ“ Successful analysis for Task 42        â† LLM baÅŸarÄ±lÄ±
[LLM] âœ— Parsing failed for Task 43, using rule-based fallback  â† Fallback
...
LLM Success Rate: 95 / 100                      â† %95 baÅŸarÄ±
Rule-Based Fallback Usage: 5 times              â† %5 fallback
```

### Semantic Decision Feed'de

```json
{
  "analysis_method": "TinyLlama (Instruction-Tuned) + Few-Shot Prompting",
  "llm_summary": "LLM Analizi: CRITICAL priority with ultra-short...",
  "reason": "Full detailed reason from LLM analysis"
}
```

---

## ğŸ”¬ Bilimsel DeÄŸer

### AraÅŸtÄ±rma SorularÄ±

1. **LLM vs Rule-Based:** Hangi karar stratejisi daha iyi sonuÃ§ verir?
   - **Metrik:** Ortalama latency, enerji tasarrufu, fairness
2. **Few-Shot EtkinliÄŸi:** Ã–rnek sayÄ±sÄ± baÅŸarÄ± oranÄ±nÄ± ne kadar artÄ±rÄ±r?
   - **Deney:** 3 Ã¶rnek vs 5 Ã¶rnek vs 10 Ã¶rnek
3. **Model Boyutu Etkisi:** Daha bÃ¼yÃ¼k model gerÃ§ekten gerekli mi?
   - **KarÅŸÄ±laÅŸtÄ±rma:** TinyLlama vs Llama 7B vs GPT-3.5

### YayÄ±n BaÅŸlÄ±klarÄ±

- "Few-Shot Prompting for IoT Task Offloading: A Study on LLM-Based Decision Making"
- "Rule-Based vs LLM-Based Semantic Analysis in Edge Computing"

---

## âš ï¸ Potansiyel Sorunlar & Ã‡Ã¶zÃ¼mler

### Sorun 1: Model YÃ¼klenmesi BaÅŸarÄ±sÄ±z

```
[LLM] Failed to load model: ...
```

**Ã‡Ã¶zÃ¼m:**

```bash
# GPU kullanÄ±labilir mi?
python -c "import torch; print(torch.cuda.is_available())"

# Manual download + cache
pip install transformers torch
huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

### Sorun 2: YÃ¼ksek Fallback OranÄ± (>20%)

**Sebep:** Few-shot Ã¶rnekleri yeterli deÄŸil veya model capacity yetersiz

**Ã‡Ã¶zÃ¼m 1:** Few-Shot'a yeni Ã¶rnekler ekle

```python
few_shot_examples += """
[EXAMPLE 4]
Input: Task Type: MIXED, ...
"""
```

**Ã‡Ã¶zÃ¼m 2:** Daha bÃ¼yÃ¼k model kullan

```python
analyzer = SemanticAnalyzer(
    use_llm=True,
    model_name="meta-llama/Llama-2-7b-hf"  # 7B model
)
```

### Sorun 3: Parsing BaÅŸarÄ±sÄ±z (TanÄ±dÄ±k olmayan Output)

**Sebep:** LLM farklÄ± format kullanÄ±yor

**Ã‡Ã¶zÃ¼m:** Prompt'u daha katÄ± yap

```python
prompt += "\nYour response format MUST be:\n"
prompt += "- Priority Score: [NUMBER]\n"
prompt += "- Recommendation: [LOCAL/EDGE/CLOUD]\n"
```

---

## ğŸ“ Ä°leri AdÄ±mlar (Future Work)

1. **Dynamic Few-Shot:** GÃ¶rev tÃ¼rÃ¼ne gÃ¶re dinamik Ã¶rnek seÃ§me
2. **In-Context Learning:** Model Ã¶nceki baÅŸarÄ±/baÅŸarÄ±sÄ±zlÄ±klardan Ã¶ÄŸrenmesi
3. **Temperature Tuning:** FarklÄ± gÃ¶revler iÃ§in temp deÄŸeri optimize etme
4. **Model Fine-Tuning:** Custom IoT gÃ¶revlerine spesifik fine-tune

---

## ğŸ“ SonuÃ§

**Yapmaya Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nÄ±z YÃ¶ntem:** âœ… Harika!

- Rule-based safety net ile LLM gÃ¼cÃ¼nÃ¼ birleÅŸtirme
- Kademeli improvement path
- Bilimsel karÅŸÄ±laÅŸtÄ±rma yapabilme
- Production-ready kod

**Beklenen Outcome:** Rule-based fallback %2-5 dÃ¼zeyine dÃ¼ÅŸecek, Ã§oÄŸu zaman TinyLlama akÄ±llÄ± kararlar verecek! ğŸš€
